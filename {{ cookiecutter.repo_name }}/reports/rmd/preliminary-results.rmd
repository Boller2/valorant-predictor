# Preliminary Results

## Objectives
1. **LFP attractor analysis.** This will help me to determine wheter Mushrom body exhibit an attractor topology with and without attention. _For this, I MUST use Van Swinderen's data_ The working hypothesis here is that __I will indeed find attractors in the "resting state" model of the LFP and the network will transit to a stable state while the attention has been fixed.__ In here I could follow the trajectory of the system.
2. **Reconstruct and analyze the attractor network.** Here I will explore the computational mechanism of the reduced model of the MB.  This will give me space for studying the properties of the network and do a proper characterization of the attractors and the trajectories that the system should follow
3. **Test the selective attention in the built network.**
    > Having a network that responds with *x* to the stimuli "A", and responds with *y* to stimuli "B", when presenting "A+B", show that the network only outputs *x*"
4. **Controlability of the attractor.** Once the network has been build and analyzed, I will try several ideas to control the stability and settle of the network. There are some papers that propose a biologically plausible mechanism for this.
5. **Replicate the experiments.** In particular, I am thinking the work of Bruno's Lab [@grabowska2020oscillations;@paulk2014selective;@van2009shared]


## What is Attention

El framework de Knudsen propone cuatro procesos fundamentales en la atención: Salience Filters, competitive selection, top-down sensitivi ty control, and working memory.
Este framework es revisado por Nityananda. Allí, él analiza la evidencia que compete con cada uno de los módulos de Knudesen. Para los filtros de saliencia, muestra que estos estan implementados principalmente por las caracteristicas de sintonización de las neuronas sensitivas (tuning properties).  Sin embargo, dicho filtrado sólo genera una mejor SNR y no dan cuenta, necesariamennte, de un proceso de atencion selectiva.  Para el caso de la selección competitiva, expone los mecanismos existentes en anisopteros, donde un mecanismo de detección de movimientos competitivo, es capaz de seleccionar el estímulo de interés

## What is the evidence of selective attention in insects?

## LFP attractor analysis {#lfp-analysis1}
The first idea to test was to reconstruct the phase space using Taken's embedding theorem.  For this, and with Jean Paul's help, I was able to build a phase space for a simple signal: A 15-secs sinusoid. $f_1 = 10Hz$ for the first 7.5 seconds and $f_2 = 37Hz$ for the last 7.5 seconds.  The results of this excersise is shown in the figure \@ref(fig:attractor1)

```python
analysis(t,fs,signal)
#Results for the noiseless case
JIDT Analysis done in 40.7085 seconds. Dopt=3.00. Tauopt=1.00
JIDT Analysis done in 14.0192 seconds. Dopt=2.00. Tauopt=1.00
JIDT Analysis done in 18.4291 seconds. Dopt=10.00. Tauopt=50.00
```

```{r attractor1, echo=FALSE, fig.cap="\\textbf{Takens Reconstruction: Noiseless case.} \\textbf{Top} shows the signal in the range that the frequency is changed. \\textbf{Middle} shows the wavelet transform, which allow to look the frequency transition in the 7.5 seconds. \\textbf{Bottom} shows the reconstruction of the space phase. Left plot is the phase reconstruction for the whole signal.  Middle for the first portion, and right for the second portion of the signal", out.width = '100%', fig.align='center'}
knitr::include_graphics('../figures/attractor1.png')
```

After this exercise, I tried with an additive gaussian noise. 
```python
noise = np.random.rand(len(y)) #y is the signal
analysis(t,fs,y+noise)
#Results for noise analysis
JIDT Analysis done in 88.512 seconds. Dopt=10.00. Tauopt=18.00
JIDT Analysis done in 41.1819 seconds. Dopt=10.00. Tauopt=24.00
JIDT Analysis done in 38.4141 seconds. Dopt=10.00. Tauopt=20.00
```

```{r attractor2, echo=FALSE, fig.cap="\\textbf{Takens Reconstruction with small noise} Same plots as the noiseless case. Notice how the two attractors tend to merge if the whole signal is analyzed (bottom left plot) ", out.width = '100%', fig.align='center'}
knitr::include_graphics('../figures/attractor2.png')
```

Finally, I tried with a huge ammount of noise
```python
noise = np.random.rand(len(y)) #y is the signal
analysis(t,fs,y+30*noise)
#Results for noise analysis
JIDT Analysis done in 100.071 seconds. Dopt=4.00. Tauopt=29.00
JIDT Analysis done in 41.8212 seconds. Dopt=8.00. Tauopt=40.00
JIDT Analysis done in 43.8605 seconds. Dopt=5.00. Tauopt=21.00
```

```{r attractor3, echo=FALSE, fig.cap="\\textbf{Takens Reconstruction with big noise} Same plots as the noiseless case. Noise is so high that even the wavelet transform is unable to distinguish the two frequencies (middle plot). Also, phase space reconstruction failed (bottom plots)", out.width = '100%', fig.align='center'}
knitr::include_graphics('../figures/attractor3.png')
```

The quick conclusion with this is that in principle is possible to reconstruct the phase space with some noise.There is a [nice web](http://www.physics.emory.edu/faculty/weeks//research/tseries1.html) which do some usefull experiments that I should try.  However, the big question is Could this work also for stochastic processes?.  Also, I should use the methods proposed in Oprisan's Paper [-@oprisan2018cocaine]


## Attractor networks 

#### Continuous attractor network

As first approach for building a CAN, I replicated the work of Miller[-@miller2016dynamical]. He uses a very simple firing rate model, based on a population of excitatory and inhibitory neurons. When the connectivity matrix is changed, different attractors appear.
First, I re-implemented the computational model proposed by Miller, translating from Matlab to Python code.  Then, I compared a fixed point attractor with a continuous linear attractor (figures 1 and 5 of his paper[@miller2016dynamical]). This comparision was done by reconstructing the space state using Taken's embedding theorem[@takens1981detecting], and plotting the trajectory of the system for 3 dimensions of the reconstructed space.


For the fixed point attractor (Fig. 1), it is possible to observe that,regardless of the perturbation, the system returns to its fixed point (red star, right plot). As the color moves from purple to yellowish, the trajectory show two cycles, originated by the input current, which moves the system out of its attractor. However, as the time pases, the system converges again to its fixed point.

Line attractor (Fig. 2) shows a smooth transition to another similar attractor, when the system is perturbed.  This is better observed in the axis projections (Fig. 2, right). As current is injected to the system, generating a movement outside its fixed attractor, there is a slight change of slope in the attractor line. However, contrary to the fixed point attractor, the system stabilizes in this new subtle attractor.  Additionally, extreme changes of colors in the 3D trajectory indicates that the system stabilized in that attractor and then moved to the other one.

```python
#Line attractor analysis
JIDT Analysis done in 165.748 seconds. Dopt=10.00. Tauopt=7.00
JIDT Analysis done in 157.843 seconds. Dopt=10.00. Tauopt=21.00
```

```{r attractor12, echo=FALSE, fig.cap="\\textbf{Takens Reconstruction for fixed point attractor.} (\\textbf{Top left}) Model and connectivity matrix is shown. The parameters used are based on the Miller's code. Injected current(\\textbf{Middle left}) and dynamical evolution for the two populations (\\textbf{bottom left}) is shown. (\\textbf{Right}) Convergence of the system to a fixed point attractor for population B. System evolution starts in the black circle. Red star shows the fixed point.", out.width = '100%', fig.align='center'}
knitr::include_graphics('../figures/fixedPoint.png')
```

```python
#Line attractor analysis
JIDT Analysis done in 168.609 seconds. Dopt=10.00. Tauopt=7.00
JIDT Analysis done in 153.953 seconds. Dopt=10.00. Tauopt=21.00
```

```{r attractor22, echo=FALSE, fig.cap="\\textbf{Takens Reconstruction for continuous line attractor} \\textbf{Left} Same as figure 2. Observe that the connectivity matrix is changed. (\\textbf{Right}) Line attractor for population B. System evolution starts in the black circle. Notice that when a stimuli arrives, the system transits through the colored line. No cycle is observed", out.width = '100%', fig.align='center'}
knitr::include_graphics('../figures/lineAttractor.png')
```

Considering the results, a phase space reconstruction is possible for a given time signal.  This reconstruction allows a better understanding of the generative system that created the time signal.  
In this particular case, generative system, time signal and phase reconstruction are known.  However, when the generative system is unknown, phase space reconstruction could be a very valuable tool for exploring the system's characteristics. **Could this approach be useful for neural stochastic processes, such as LFP?**[@oprisan2018cocaine]. 
\newpage